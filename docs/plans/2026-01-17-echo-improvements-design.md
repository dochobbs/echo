# Echo Improvements Design

**Date:** 2026-01-17
**Status:** Approved
**Author:** Design session with Claude

---

## Context

Echo is a live AI attending/tutor for pediatric medical education with a small user base (~10 learners). Users include medical students, residents, and NP/PA students.

Current pain points:
- Limited case variety (same presentations)
- Debrief quality and accessibility
- No admin visibility into learner usage
- Unused features (describe-a-case)

---

## Priority Sequence

| Phase | Feature | Rationale |
|-------|---------|-----------|
| 1 | Remove describe-a-case | Dead code cleanup |
| 2 | Expand conditions | Immediate variety for users |
| 3 | Improve debriefs | Complete the learning loop |
| 4 | Admin dashboard | Visibility as user base grows |
| 5 | C-CDA import | Advanced feature, can wait |

---

## Phase 1: Remove Describe-a-Case

**Approach:** Comment out (not delete) to preserve option to restore.

**Files affected:**
- `src/cases/router.py` - Comment out `/describe/start` and `/describe/message` endpoints
- Add note: `# DISABLED 2026-01 - describe-a-case on hold`

**Keep intact:**
- Models in `src/cases/models.py` (no import errors)
- Methods in `src/core/tutor.py` (won't be called)
- `src/core/citations.py` (may use for debrief reading lists)

---

## Phase 2: Expand Conditions via Dynamic Parameters

**Approach:** Enhance `StartCaseRequest` with variant parameters rather than multiplying YAML files.

**New parameters:**

```python
class StartCaseRequest(BaseModel):
    learner_level: LearnerLevel = LearnerLevel.STUDENT
    condition_key: Optional[str] = None
    time_constraint: Optional[int] = None

    # Variant controls
    severity: Optional[Literal["mild", "moderate", "severe"]] = None
    age_bracket: Optional[Literal["neonate", "infant", "toddler", "child", "adolescent"]] = None
    presentation: Optional[Literal["typical", "atypical", "early", "late"]] = None
    complexity: Optional[Literal["straightforward", "nuanced", "challenging"]] = None
```

**Behavior:**
- `None` = Claude picks randomly (current behavior)
- Specified value = Claude generates matching variant
- `complexity` defaults to learner level but can be overridden

**Prompt enhancement:**
Add section to `dynamic_generator.py` `_generate_patient()`:

```
## Case Variant Parameters
- Severity: {severity or "your choice"}
- Age bracket: {age_bracket or "within framework range"}
- Presentation: {presentation or "your choice"}
- Complexity: {complexity or "appropriate for learner level"}
```

---

## Phase 3: Debrief Improvements

### 3a: AI Attending Quality

Current prompts are working well. Minor tweaks only, no major changes.

### 3b: Debrief Access & Post-Debrief Q&A

**Current state:**
- Debriefs generated once at case end
- Stored in `CaseExport` with structured data
- Not easily accessible after completion

**New endpoints:**

| Endpoint | Purpose |
|----------|---------|
| `GET /case/{session_id}/debrief` | Retrieve structured debrief |
| `POST /case/{session_id}/question` | Ask follow-up question with case context |

**Post-debrief Q&A flow:**
1. Learner completes case, sees debrief
2. Later, revisits debrief from history
3. Asks: "Why was amoxicillin better than azithromycin here?"
4. Echo responds with full case context

**Implementation:**
- Load `CaseExport` by session_id
- Build conversation with debrief + case transcript as context
- Use existing Tutor class for response generation

---

## Phase 4: Admin Dashboard

### Auth Changes

Add role field to User model:

```python
class User:
    # ... existing fields ...
    role: Literal["learner", "admin"] = "learner"
```

### New Endpoints

All require `role == "admin"`:

| Endpoint | Returns |
|----------|---------|
| `GET /admin/users` | All users with stats summary |
| `GET /admin/users/{id}` | Single user detail + case history |
| `GET /admin/cases` | All cases (filterable by user, condition, date) |
| `GET /admin/cases/{session_id}` | Full case detail including transcript |
| `GET /admin/metrics` | Platform-wide aggregates |
| `GET /admin/metrics/struggles` | Common stuck points, hint patterns |

### Metrics Payload

```json
{
  "total_users": 8,
  "active_last_7_days": 5,
  "total_cases": 47,
  "completed_cases": 41,
  "avg_case_duration_minutes": 12.3,
  "most_practiced_conditions": ["otitis_media", "croup", "bronchiolitis"],
  "avg_hints_per_case": 1.2,
  "completion_rate": 0.87
}
```

### Struggles Payload

```json
{
  "common_stuck_phases": ["assessment", "plan"],
  "high_hint_conditions": ["febrile_infant", "meningitis"],
  "frequent_missed_items": ["checking allergies", "weight-based dosing"]
}
```

---

## Phase 5: C-CDA Patient Import

### Use Case

Synthetic patient panels generated by Oread, imported into Echo for case practice. NOT real patient data (no HIPAA concerns).

### Flow

1. Oread generates synthetic patient with C-CDA export
2. Learner uploads C-CDA file to Echo
3. Echo parses into patient context
4. Learner runs cases using that patient's history/problems/meds

### New Endpoints

| Endpoint | Purpose |
|----------|---------|
| `POST /patients/import` | Upload C-CDA, returns parsed patient |
| `GET /patients` | List user's imported patients |
| `GET /patients/{id}` | Get patient detail |
| `DELETE /patients/{id}` | Remove from panel |

### Case Integration

Add optional `patient_id` to `StartCaseRequest`:

```json
{
  "condition_key": "asthma",
  "learner_level": "resident",
  "patient_id": "abc-123"
}
```

Dynamic generator incorporates patient's real context:
- Existing conditions from problem list
- Current medications
- Known allergies
- Recent encounter history

### Parser Approach

- Use `lxml` to parse C-CDA XML
- Extract standard sections: problems, meds, allergies, encounters
- Map to `PatientContext` model
- Store parsed data in DB (not raw XML)

---

## Future Opportunities

Noted during design review, not in current scope:

| Opportunity | Description |
|-------------|-------------|
| **Confidence tracking** | Pre/post confidence ratings per case |
| **Spaced repetition** | Suggest condition review based on time + scores |
| **Cohort features** | Leaderboards, peer comparisons |
| **Mobile/voice-first** | On-the-go learning experience |
| **Clinical accuracy guardrails** | Spot-check generated cases against framework red flags |

---

## Risk Notes

- **AI attending quality is everything** - Protect what's working, A/B test prompt changes
- **Small user base** - Can move fast, but get direct feedback
- **C-CDA complexity** - Parser may need iteration; start with happy path

---

## Implementation Order

1. Phase 1: Comment out describe-a-case
2. Phase 2: Add variant parameters to StartCaseRequest
3. Phase 3: Debrief endpoints + post-debrief Q&A
4. Phase 4: Admin role + dashboard endpoints
5. Phase 5: C-CDA parser + patient import
